{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae2066f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p2 bigram, trigram, ngram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "907147da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', \"'Hello\"), (\"'Hello\", 'everyone'), ('everyone', '.'), ('.', \"'\"), (\"'\", ','), (',', \"'This\"), (\"'This\", 'is'), ('is', 'an'), ('an', 'introducton'), ('introducton', 'to'), ('to', 'tokenization'), ('tokenization', '.'), ('.', \"'\"), (\"'\", ','), (',', \"'Sit\"), (\"'Sit\", 'back'), ('back', ','), (',', 'and'), ('and', 'relax'), ('relax', ','), (',', 'enjoy'), ('enjoy', 'your'), ('your', 'first'), ('first', 'lesson'), ('lesson', 'of'), ('of', 'NLP'), ('NLP', ']')]\n",
      "\n",
      "\n",
      "[('[', \"'Hello\", 'everyone'), (\"'Hello\", 'everyone', '.'), ('everyone', '.', \"'\"), ('.', \"'\", ','), (\"'\", ',', \"'This\"), (',', \"'This\", 'is'), (\"'This\", 'is', 'an'), ('is', 'an', 'introducton'), ('an', 'introducton', 'to'), ('introducton', 'to', 'tokenization'), ('to', 'tokenization', '.'), ('tokenization', '.', \"'\"), ('.', \"'\", ','), (\"'\", ',', \"'Sit\"), (',', \"'Sit\", 'back'), (\"'Sit\", 'back', ','), ('back', ',', 'and'), (',', 'and', 'relax'), ('and', 'relax', ','), ('relax', ',', 'enjoy'), (',', 'enjoy', 'your'), ('enjoy', 'your', 'first'), ('your', 'first', 'lesson'), ('first', 'lesson', 'of'), ('lesson', 'of', 'NLP'), ('of', 'NLP', ']')]\n",
      "\n",
      "\n",
      "[('[', \"'Hello\", 'everyone', '.'), (\"'Hello\", 'everyone', '.', \"'\"), ('everyone', '.', \"'\", ','), ('.', \"'\", ',', \"'This\"), (\"'\", ',', \"'This\", 'is'), (',', \"'This\", 'is', 'an'), (\"'This\", 'is', 'an', 'introducton'), ('is', 'an', 'introducton', 'to'), ('an', 'introducton', 'to', 'tokenization'), ('introducton', 'to', 'tokenization', '.'), ('to', 'tokenization', '.', \"'\"), ('tokenization', '.', \"'\", ','), ('.', \"'\", ',', \"'Sit\"), (\"'\", ',', \"'Sit\", 'back'), (',', \"'Sit\", 'back', ','), (\"'Sit\", 'back', ',', 'and'), ('back', ',', 'and', 'relax'), (',', 'and', 'relax', ','), ('and', 'relax', ',', 'enjoy'), ('relax', ',', 'enjoy', 'your'), (',', 'enjoy', 'your', 'first'), ('enjoy', 'your', 'first', 'lesson'), ('your', 'first', 'lesson', 'of'), ('first', 'lesson', 'of', 'NLP'), ('lesson', 'of', 'NLP', ']')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample=\"['Hello everyone.', 'This is an introducton to tokenization.','Sit back, and relax, enjoy your first lesson of NLP]\"\n",
    "\n",
    "sample_tokens = word_tokenize (sample)\n",
    "#applying tokenization by passing it to word_tokenize\n",
    "sample_tokens\n",
    "\n",
    "#applying bigrams\n",
    "print(list(nltk.bigrams(sample_tokens)))\n",
    "print(\"\\n\")\n",
    "\n",
    "#applying trigrams\n",
    "print(list(nltk.trigrams(sample_tokens)))\n",
    "print(\"\\n\")\n",
    "\n",
    "#applying n-grams\n",
    "print(list(nltk.ngrams(sample_tokens,4)))\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd934904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
